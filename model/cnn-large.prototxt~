name: "lnet"
force_backward: true
input: "data"
input_dim: 1
input_dim: 512
input_dim: 46 
input_dim: 46 
input: "data1"
input_dim: 1
input_dim: 512
input_dim: 46 
input_dim: 46 
input: "data2"
input_dim: 1
input_dim: 512
input_dim: 46 
input_dim: 46 


layer { 
  bottom: "data"
  top: "drop_data"
  name: "drop_data"
  type: "Dropout"
  dropout_param {
    dropout_ratio: 0.
  }
}

layer {
  bottom: "drop_data"
  top: "conv5_f1"
  name: "conv5_f1"
  type: "Wtf"
  #type: "Convolution"
  param {
    lr_mult: 0.01
    decay_mult: 5
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 100
    pad: 2
   # mask: true
    mylayerone: true
    kernel_size: 5
    weight_filler {
      type: "gaussian"
      std: 1e-7 #1e-7
    }
    bias_filler {
      type: "constant"
      value: 0.1
    }
  }
}

layer {
  bottom: "conv5_f1"
  top: "conv5_f2"
  name: "conv5_f2"
  type: "Wtfeighth"
 #type: "Convolution"
  param {
    #lr_mult: 1000000
    lr_mult: 2000000
    #lr_mult: 0
    decay_mult: 5
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    mylayertwo: true
    num_output: 100
    group: 1
    pad: 1
    kernel_size: 3
    weight_filler {
      type: "gaussian"
      std: 1e-7
    }
    bias_filler {
      type: "constant"
      value: 0
    }
  }
}
